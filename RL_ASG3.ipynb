{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKqYqN3274fv",
        "outputId": "e60d3883-1d0a-4a99-aeae-5ada856f0fea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Total Loss: -0.1762932687997818\n",
            "Episode: 1, Total Loss: 10.267303466796875\n",
            "Episode: 2, Total Loss: 149.88095092773438\n",
            "Episode: 3, Total Loss: -2.6932926177978516\n",
            "Episode: 4, Total Loss: 27.903669357299805\n",
            "Episode: 5, Total Loss: 6.8168182373046875\n",
            "Episode: 6, Total Loss: -0.8838443756103516\n",
            "Episode: 7, Total Loss: 2.848550796508789\n",
            "Episode: 8, Total Loss: 2.001223087310791\n",
            "Episode: 9, Total Loss: 20.46839714050293\n",
            "Episode: 10, Total Loss: -0.8348433971405029\n",
            "Episode: 11, Total Loss: -2.1835055351257324\n",
            "Episode: 12, Total Loss: -1.3538122177124023\n",
            "Episode: 13, Total Loss: 450.0263977050781\n",
            "Episode: 14, Total Loss: 0.741605281829834\n",
            "Episode: 15, Total Loss: 4.183908462524414\n",
            "Episode: 16, Total Loss: -1.9500892162322998\n",
            "Episode: 17, Total Loss: -4.374029636383057\n",
            "Episode: 18, Total Loss: -1.9036917686462402\n",
            "Episode: 19, Total Loss: 13.709814071655273\n",
            "Episode: 20, Total Loss: 3.1658968925476074\n",
            "Episode: 21, Total Loss: 2.9143762588500977\n",
            "Episode: 22, Total Loss: 336.4642028808594\n",
            "Episode: 23, Total Loss: 181.11181640625\n",
            "Episode: 24, Total Loss: -0.13071656227111816\n",
            "Episode: 25, Total Loss: -1.803667426109314\n",
            "Episode: 26, Total Loss: -1.333912968635559\n",
            "Episode: 27, Total Loss: 53.63875961303711\n",
            "Episode: 28, Total Loss: 4.409646987915039\n",
            "Episode: 29, Total Loss: -1.3752503395080566\n",
            "Episode: 30, Total Loss: 27.917728424072266\n",
            "Episode: 31, Total Loss: -2.5250298976898193\n",
            "Episode: 32, Total Loss: 37.80004119873047\n",
            "Episode: 33, Total Loss: 0.828303337097168\n",
            "Episode: 34, Total Loss: -1.3229937553405762\n",
            "Episode: 35, Total Loss: 0.5634464025497437\n",
            "Episode: 36, Total Loss: -5.173768043518066\n",
            "Episode: 37, Total Loss: -0.5254930257797241\n",
            "Episode: 38, Total Loss: -1.48834228515625\n",
            "Episode: 39, Total Loss: -1.0686204433441162\n",
            "Episode: 40, Total Loss: 1.9473063945770264\n",
            "Episode: 41, Total Loss: -1.4146537780761719\n",
            "Episode: 42, Total Loss: 6.7328314781188965\n",
            "Episode: 43, Total Loss: 6.461548805236816\n",
            "Episode: 44, Total Loss: 0.7096471786499023\n",
            "Episode: 45, Total Loss: 3.8432459831237793\n",
            "Episode: 46, Total Loss: 1.5556440353393555\n",
            "Episode: 47, Total Loss: -0.9930505752563477\n",
            "Episode: 48, Total Loss: 30.410730361938477\n",
            "Episode: 49, Total Loss: -4.107681751251221\n",
            "Episode: 50, Total Loss: -0.8939828872680664\n",
            "Episode: 51, Total Loss: 6.0136942863464355\n",
            "Episode: 52, Total Loss: 0.7082959413528442\n",
            "Episode: 53, Total Loss: -2.593669891357422\n",
            "Episode: 54, Total Loss: -0.4093506932258606\n",
            "Episode: 55, Total Loss: 5.396175384521484\n",
            "Episode: 56, Total Loss: 3.530649185180664\n",
            "Episode: 57, Total Loss: 24.840835571289062\n",
            "Episode: 58, Total Loss: 3.7965784072875977\n",
            "Episode: 59, Total Loss: 2.7809994220733643\n",
            "Episode: 60, Total Loss: 1.1891069412231445\n",
            "Episode: 61, Total Loss: -1.586390495300293\n",
            "Episode: 62, Total Loss: 0.6182737350463867\n",
            "Episode: 63, Total Loss: 20.155902862548828\n",
            "Episode: 64, Total Loss: 1.9353961944580078\n",
            "Episode: 65, Total Loss: -0.5204057693481445\n",
            "Episode: 66, Total Loss: -2.4874556064605713\n",
            "Episode: 67, Total Loss: 0.5079247951507568\n",
            "Episode: 68, Total Loss: 1.0291118621826172\n",
            "Episode: 69, Total Loss: 0.10891056060791016\n",
            "Episode: 70, Total Loss: 1.0002937316894531\n",
            "Episode: 71, Total Loss: -1.0558089017868042\n",
            "Episode: 72, Total Loss: 0.009179294109344482\n",
            "Episode: 73, Total Loss: 9.387918472290039\n",
            "Episode: 74, Total Loss: -0.04852771759033203\n",
            "Episode: 75, Total Loss: -1.20149564743042\n",
            "Episode: 76, Total Loss: -1.5468074083328247\n",
            "Episode: 77, Total Loss: -2.173814296722412\n",
            "Episode: 78, Total Loss: -0.6838210821151733\n",
            "Episode: 79, Total Loss: -2.1077969074249268\n",
            "Episode: 80, Total Loss: 5.808452606201172\n",
            "Episode: 81, Total Loss: -0.8502180576324463\n",
            "Episode: 82, Total Loss: -0.9346883893013\n",
            "Episode: 83, Total Loss: 0.4169408082962036\n",
            "Episode: 84, Total Loss: -0.5834412574768066\n",
            "Episode: 85, Total Loss: -4.7531657218933105\n",
            "Episode: 86, Total Loss: 0.08898162841796875\n",
            "Episode: 87, Total Loss: -1.8576689958572388\n",
            "Episode: 88, Total Loss: -5.176702976226807\n",
            "Episode: 89, Total Loss: -2.9253482818603516\n",
            "Episode: 90, Total Loss: 3.2821044921875\n",
            "Episode: 91, Total Loss: -0.6197929382324219\n",
            "Episode: 92, Total Loss: 16.427047729492188\n",
            "Episode: 93, Total Loss: -1.107714056968689\n",
            "Episode: 94, Total Loss: -0.4870743751525879\n",
            "Episode: 95, Total Loss: 6.046352863311768\n",
            "Episode: 96, Total Loss: 1.8861045837402344\n",
            "Episode: 97, Total Loss: -4.43133544921875\n",
            "Episode: 98, Total Loss: -1.1735000610351562\n",
            "Episode: 99, Total Loss: 9.126440048217773\n",
            "Episode: 100, Total Loss: 2.9890811443328857\n",
            "Episode: 101, Total Loss: -1.5113849639892578\n",
            "Episode: 102, Total Loss: -2.9833333492279053\n",
            "Episode: 103, Total Loss: -0.7524699568748474\n",
            "Episode: 104, Total Loss: -5.615077972412109\n",
            "Episode: 105, Total Loss: -1.2927120923995972\n",
            "Episode: 106, Total Loss: -0.8845517635345459\n",
            "Episode: 107, Total Loss: -0.7267100811004639\n",
            "Episode: 108, Total Loss: -0.3347809314727783\n",
            "Episode: 109, Total Loss: 16.583724975585938\n",
            "Episode: 110, Total Loss: -1.4958889484405518\n",
            "Episode: 111, Total Loss: -3.7276055812835693\n",
            "Episode: 112, Total Loss: -2.3453316688537598\n",
            "Episode: 113, Total Loss: -1.127204418182373\n",
            "Episode: 114, Total Loss: -2.0374526977539062\n",
            "Episode: 115, Total Loss: -1.8279458284378052\n",
            "Episode: 116, Total Loss: -0.7381634712219238\n",
            "Episode: 117, Total Loss: 5.282018661499023\n",
            "Episode: 118, Total Loss: -1.469956636428833\n",
            "Episode: 119, Total Loss: -2.8265867233276367\n",
            "Episode: 120, Total Loss: -2.809174060821533\n",
            "Episode: 121, Total Loss: -0.5944459438323975\n",
            "Episode: 122, Total Loss: -2.338597536087036\n",
            "Episode: 123, Total Loss: -4.003661632537842\n",
            "Episode: 124, Total Loss: -4.749077320098877\n",
            "Episode: 125, Total Loss: -2.4486658573150635\n",
            "Episode: 126, Total Loss: 0.19098961353302002\n",
            "Episode: 127, Total Loss: -5.2483344078063965\n",
            "Episode: 128, Total Loss: -0.5304529666900635\n",
            "Episode: 129, Total Loss: -5.0724663734436035\n",
            "Episode: 130, Total Loss: -3.8920235633850098\n",
            "Episode: 131, Total Loss: 2.4294469356536865\n",
            "Episode: 132, Total Loss: -9.6931791305542\n",
            "Episode: 133, Total Loss: -0.8897314071655273\n",
            "Episode: 134, Total Loss: -3.7257609367370605\n",
            "Episode: 135, Total Loss: -0.6192309856414795\n",
            "Episode: 136, Total Loss: -1.6574819087982178\n",
            "Episode: 137, Total Loss: -1.5322309732437134\n",
            "Episode: 138, Total Loss: -4.831057548522949\n",
            "Episode: 139, Total Loss: 24.749197006225586\n",
            "Episode: 140, Total Loss: -3.718630313873291\n",
            "Episode: 141, Total Loss: 7.038369178771973\n",
            "Episode: 142, Total Loss: -0.5143871307373047\n",
            "Episode: 143, Total Loss: -3.567495822906494\n",
            "Episode: 144, Total Loss: -0.29946231842041016\n",
            "Episode: 145, Total Loss: 34.93119430541992\n",
            "Episode: 146, Total Loss: -2.9862804412841797\n",
            "Episode: 147, Total Loss: 0.3001539707183838\n",
            "Episode: 148, Total Loss: 5.224320411682129\n",
            "Episode: 149, Total Loss: -1.188276767730713\n",
            "Episode: 150, Total Loss: -3.380434989929199\n",
            "Episode: 151, Total Loss: -2.2451868057250977\n",
            "Episode: 152, Total Loss: -1.0429725646972656\n",
            "Episode: 153, Total Loss: 7.003631591796875\n",
            "Episode: 154, Total Loss: 4.252521991729736\n",
            "Episode: 155, Total Loss: 2.121011972427368\n",
            "Episode: 156, Total Loss: -0.7949095964431763\n",
            "Episode: 157, Total Loss: -1.254077672958374\n",
            "Episode: 158, Total Loss: 0.13440585136413574\n",
            "Episode: 159, Total Loss: -3.115579605102539\n",
            "Episode: 160, Total Loss: 0.4256625175476074\n",
            "Episode: 161, Total Loss: 0.7007828950881958\n",
            "Episode: 162, Total Loss: 0.4944554567337036\n",
            "Episode: 163, Total Loss: -2.2211172580718994\n",
            "Episode: 164, Total Loss: -0.6485258340835571\n",
            "Episode: 165, Total Loss: 2.105851888656616\n",
            "Episode: 166, Total Loss: 1.8205068111419678\n",
            "Episode: 167, Total Loss: -0.4038175344467163\n",
            "Episode: 168, Total Loss: -0.18775534629821777\n",
            "Episode: 169, Total Loss: 0.07748618721961975\n",
            "Episode: 170, Total Loss: -0.21538317203521729\n",
            "Episode: 171, Total Loss: -0.20477962493896484\n",
            "Episode: 172, Total Loss: 6.17513370513916\n",
            "Episode: 173, Total Loss: -3.687723398208618\n",
            "Episode: 174, Total Loss: -0.8060284852981567\n",
            "Episode: 175, Total Loss: -0.04362136125564575\n",
            "Episode: 176, Total Loss: -1.392319679260254\n",
            "Episode: 177, Total Loss: -2.1775403022766113\n",
            "Episode: 178, Total Loss: 2.6971383094787598\n",
            "Episode: 179, Total Loss: -1.4694451093673706\n",
            "Episode: 180, Total Loss: 3.062384605407715\n",
            "Episode: 181, Total Loss: 41.49992370605469\n",
            "Episode: 182, Total Loss: 11.500853538513184\n",
            "Episode: 183, Total Loss: 2.794221878051758\n",
            "Episode: 184, Total Loss: -0.5327916145324707\n",
            "Episode: 185, Total Loss: 15.180610656738281\n",
            "Episode: 186, Total Loss: -2.615510940551758\n",
            "Episode: 187, Total Loss: 1.281761646270752\n",
            "Episode: 188, Total Loss: 0.27444958686828613\n",
            "Episode: 189, Total Loss: -2.1537723541259766\n",
            "Episode: 190, Total Loss: -2.4097917079925537\n",
            "Episode: 191, Total Loss: -1.251024842262268\n",
            "Episode: 192, Total Loss: -0.6359801292419434\n",
            "Episode: 193, Total Loss: -3.5255119800567627\n",
            "Episode: 194, Total Loss: -1.456949234008789\n",
            "Episode: 195, Total Loss: -2.592548370361328\n",
            "Episode: 196, Total Loss: 1.0229601860046387\n",
            "Episode: 197, Total Loss: 0.7134498357772827\n",
            "Episode: 198, Total Loss: 0.3861882984638214\n",
            "Episode: 199, Total Loss: -3.0259194374084473\n",
            "Current Board:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "#Import needed Libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the Tic-Tac-Toe environment\n",
        "class TicTacToeEnvironment:\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((3, 3), dtype=np.int32)\n",
        "        self.current_player = 1\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((3, 3), dtype=np.int32)\n",
        "        self.current_player = 1\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.board.flatten()\n",
        "\n",
        "    def make_move(self, move):\n",
        "        row, col = move\n",
        "        if self.board[row, col] == 0:\n",
        "            self.board[row, col] = self.current_player\n",
        "            self.current_player = -self.current_player\n",
        "\n",
        "    def is_winner(self):\n",
        "        for player in [1, -1]:\n",
        "            # Check rows and columns\n",
        "            if np.any(np.all(self.board == player, axis=0)) or np.any(np.all(self.board == player, axis=1)):\n",
        "                return player\n",
        "            # Check diagonals\n",
        "            if np.all(np.diag(self.board) == player) or np.all(np.diag(np.fliplr(self.board)) == player):\n",
        "                return player\n",
        "        return 0\n",
        "\n",
        "    def is_draw(self):\n",
        "        return np.all(self.board != 0)\n",
        "\n",
        "# Define the Actor-Critic model\n",
        "class ActorCriticModel(tf.keras.Model):\n",
        "    def __init__(self, num_actions):\n",
        "        super(ActorCriticModel, self).__init__()\n",
        "        self.dense = tf.keras.layers.Dense(128, activation='relu')\n",
        "        self.policy = tf.keras.layers.Dense(num_actions, activation='softmax')\n",
        "        self.value = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.dense(state)\n",
        "        policy = self.policy(x)\n",
        "        value = self.value(x)\n",
        "        return policy, value\n",
        "\n",
        "# Train the Actor-Critic model\n",
        "def train_actor_critic(model, optimizer, states, actions, rewards):\n",
        "    states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "    actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
        "    rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        policy, values = model(states)\n",
        "        values = tf.squeeze(values)\n",
        "        advantages = rewards - values\n",
        "\n",
        "        action_masks = tf.one_hot(actions, depth=num_actions)\n",
        "        selected_probs = tf.reduce_sum(action_masks * policy, axis=1)\n",
        "\n",
        "        policy_loss = -tf.math.log(selected_probs + 1e-8) * advantages\n",
        "        value_loss = tf.square(advantages)\n",
        "\n",
        "        total_loss = tf.reduce_sum(policy_loss) + tf.reduce_sum(value_loss)\n",
        "\n",
        "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return total_loss\n",
        "\n",
        "# Play Tic-Tac-Toe against the trained model\n",
        "def play_with_trained_model(model):\n",
        "    env = TicTacToeEnvironment()\n",
        "    while True:\n",
        "        print(\"Current Board:\")\n",
        "        print(env.board)\n",
        "\n",
        "        if env.current_player == 1:\n",
        "            # Human player's turn\n",
        "            row = int(input(\"Enter the row (0, 1, 2): \"))\n",
        "            col = int(input(\"Enter the column (0, 1, 2): \"))\n",
        "            move = (row, col)\n",
        "        else:\n",
        "            # Model's turn\n",
        "            state = env.get_state()\n",
        "            policy, _ = model(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "            policy = np.squeeze(policy.numpy())\n",
        "            action = np.random.choice(num_actions, p=policy)\n",
        "            move = (action // 3, action % 3)\n",
        "\n",
        "        env.make_move(move)\n",
        "\n",
        "        winner = env.is_winner()\n",
        "        if winner != 0:\n",
        "            print(\"Game Over!\")\n",
        "            if winner == 1:\n",
        "                print(\"You Win!\")\n",
        "            else:\n",
        "                print(\"Model Wins!\")\n",
        "            break\n",
        "\n",
        "        if env.is_draw():\n",
        "            print(\"Game Over! It's a Draw!\")\n",
        "            break\n",
        "\n",
        "# Create Tic-Tac-Toe environment and Actor-Critic model\n",
        "env = TicTacToeEnvironment()\n",
        "num_actions = 9\n",
        "actor_critic_model = ActorCriticModel(num_actions)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
        "\n",
        "# Training parameters\n",
        "num_episodes = 200\n",
        "epsilon = 0.1\n",
        "\n",
        "# Training loop\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    episode_states, episode_actions, episode_rewards = [], [], []\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        for step in range(9):  # Max number of steps in Tic-Tac-Toe\n",
        "            state = env.get_state()\n",
        "            episode_states.append(state)\n",
        "\n",
        "            # Forward pass\n",
        "            policy, value = actor_critic_model(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "\n",
        "            # Add a small constant to the probabilities to avoid NaN\n",
        "            policy += 1e-8\n",
        "\n",
        "            # Normalize the probabilities to sum to 1\n",
        "            policy /= tf.reduce_sum(policy)\n",
        "\n",
        "            action = np.random.choice(num_actions, p=np.squeeze(policy))\n",
        "            episode_actions.append(action)\n",
        "\n",
        "            # Apply action to the environment\n",
        "            env.make_move((action // 3, action % 3))\n",
        "\n",
        "            # Calculate reward\n",
        "            reward = env.is_winner()\n",
        "            if reward == 0 and env.is_draw():\n",
        "                reward = 0.1  # Slightly positive reward for a draw\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "            if reward != 0:\n",
        "                break\n",
        "\n",
        "        total_loss = train_actor_critic(actor_critic_model, optimizer, episode_states, episode_actions, episode_rewards)\n",
        "        print(f\"Episode: {episode}, Total Loss: {total_loss.numpy()}\")\n",
        "\n",
        "# After training, play the game against the trained model\n",
        "play_with_trained_model(actor_critic_model)\n"
      ]
    }
  ]
}